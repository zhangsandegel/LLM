[TOC]



# 大模型面经

## 网友面经汇总

### 1. 阿里云瓴羊大模型算法一面 时长：45min

#### 1. 为啥先sft再grpo

> 要点：先让模型“会说”，再让它“说得对”。
>
> - **SFT**（监督微调）：用高质量指令/示例让模型学到**格式、基本对齐与可用性**，稳定起步、降低 RL 难度。
> - **GRPO**：在会说的前提上，用奖励去**优化目标行为**（正确率、可执行性、遵从规范等），避免 RL 从零噪声起步。
> - 先 SFT 可显著**提高采样质量**，让 RL 的探索空间更小、收敛更稳，减少崩坏/跑偏。
>

#### 2. grpo的loss函数?奖励加在哪的?

#### 3. dsr1是咋训练出来的?dsr1的奖励是这样的?在哪个阶段? 

#### 4. ppo的loss函数?奖励是加在哪的? 

#### 5. 过拟合欠拟合，大参数模型会过拟合还是欠拟合?原因? 

#### 6.反向传播的时候梯度是如何一层一层计算的?

####  7.lora微调的层数，每个层数的参数减到多少了?

####  8.了解k–means，svm，xgboost嘛？ 

#### 9.deepspeed三阶段? 

#### 10.模型为啥要量化部署?一般咋量化部署? 

#### 11.bf16和fp16的区别? 

#### 12.大模型之后咋发展? 

#### 13.tob大模型应用场景熟悉吗?