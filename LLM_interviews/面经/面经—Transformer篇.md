[TOC]



# 面经—Transformer篇

# 字节一面

### 1. transformer encoder结构 位置编码用的是什么 除了正余弦位置编码还有什么别的

### 2. 具体讲multi head attention怎么做的以及公式 为什么要除根号下dk

### 3. transformer decoder和encoder有哪些不同的地方

### 4. Cross4 attention中q k v分别来自哪里？

### 5. 带mask的self attention是什么样的mask

### 6. 为什么transformer中用的是ln不是bn

### 7.交叉熵损失函数公式 为什么分类问题只能用交叉熵不能用mse

## 代码题：

### 1. 交叉熵损失函数公式写出来

### 2. 求y=wx+b梯度

### 3. 两个数组的子序列求最大点积 有点难 用二维dp才行



# 快手star二面

#### 1.写了self attention

#### 2.为什么要除根号下dk，能不能不是根号下dk是别的？(从数学原理上答)

#### 3. multi head attention怎么做的